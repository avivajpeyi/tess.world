{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESS Atlas fit for TOI {{{TOINUMBER}}}\n",
    "\n",
    "**Version: {{{VERSIONNUMBER}}}**\n",
    "\n",
    "**Note: This notebook was automatically generated as part of the TESS Atlas project. More information can be found on GitHub:** [github.com/dfm/tess-atlas](https://github.com/dfm/tess-atlas)\n",
    "\n",
    "In this notebook, we do a full probabilistic characterization of the TESS Objects of Interest (TOIs) in the system number {{{TOINUMBER}}}.\n",
    "To do this fit, we use the [exoplanet](https://docs.exoplanet.codes) library and you can find more information about that project at [docs.exoplanet.codes](https://docs.exoplanet.codes).\n",
    "\n",
    "From here, you can scroll down and take a look at the fit results, or you can:\n",
    "\n",
    "- [open the notebook in Google Colab to run the fit yourself](https://colab.research.google.com/github/dfm/tess-atlas/blob/gh-pages/notebooks/{{{VERSIONNUMBER}}}/toi-{{{TOINUMBER}}}.ipynb),\n",
    "- [view the notebook on GitHub](https://github.com/dfm/tess-atlas/blob/gh-pages/notebooks/{{{VERSIONNUMBER}}}/toi-{{{TOINUMBER}}}.ipynb), or\n",
    "- [download the notebook](https://github.com/dfm/tess-atlas/raw/gh-pages/notebooks/{{{VERSIONNUMBER}}}/toi-{{{TOINUMBER}}}.ipynb).\n",
    "\n",
    "## Caveats\n",
    "\n",
    "There are many caveats associated with an automated bulk analysis that should be kept in mind.\n",
    "Here are some things that come to mind:\n",
    "\n",
    "1. Transit timing variations, correlated noise, and (probably) your favorite systematics are ignored. Sorry!\n",
    "\n",
    "2. This notebook was generated automatically without human intervention. Use at your own risk!\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Getting started](#Getting-started)\n",
    "2. [Data access and pre-processing](#Data-access-and-pre-processing)\n",
    "3. [The probabilistic model and initialization](#The-probabilistic-model-and-initialization)\n",
    "5. [Inference](#Inference)\n",
    "6. [Results](#Results)\n",
    "7. [Attribution and environment](#Attribution-and-environment)\n",
    "\n",
    "## Getting started\n",
    "\n",
    "To get going, we'll need to make out plots show up inline and import all the required packages.\n",
    "Note that this notebook is mostly self contained, but for technical reasons, there are a few helper functions implemented in the [tess_world package](https://github.com/exoplanet-dev/tess.world) so, if you're running this notebook locally, you'll need to install that and its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import pathlib\n",
    "\n",
    "import h5py\n",
    "import corner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightkurve as lk\n",
    "import matplotlib.pyplot as plt\n",
    "from astroquery.mast import Observations\n",
    "\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "\n",
    "import exoplanet as xo\n",
    "\n",
    "import tess_world\n",
    "tess_world.setup_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data access and pre-processing\n",
    "\n",
    "Now we will get the information about this TOI from the [Exoplanet Archive TOI table](https://exoplanetarchive.ipac.caltech.edu/docs/API_toi_columns.html).\n",
    "These parameters will be used as an initial guess for the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toi_num = 514\n",
    "\n",
    "tois = tess_world.get_toi_list()\n",
    "\n",
    "# Select all of the rows in the TOI table that are associated with this target\n",
    "toi = tois[tois.toi == toi_num + 0.01]\n",
    "if not len(toi):\n",
    "    raise RuntimeError(f\"no TOI entry for {toi_num}\")\n",
    "toi = toi.iloc[0]\n",
    "tic = toi.tid\n",
    "tois = tois[tois.tid == tic].sort_values(\"toi\")\n",
    "num_toi = len(tois)\n",
    "\n",
    "# Extract the planet periods\n",
    "period_guess = np.array(tois.pl_orbper, dtype=float)\n",
    "\n",
    "# Convert the phase to TBJD from BJD\n",
    "t0_guess = np.array(tois.pl_tranmid, dtype=float) - 2457000\n",
    "\n",
    "# Convert the depth to parts per thousand from parts per million\n",
    "depth_guess = 1e-3 * np.array(tois.pl_trandep, dtype=float)\n",
    "\n",
    "# Convert the duration to days from hours\n",
    "duration_guess = np.array(tois.pl_trandurh, dtype=float) / 24.0\n",
    "\n",
    "tois[[\"tid\", \"toi\", \"pl_orbper\", \"pl_trandep\", \"pl_trandurh\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can search for and download the light curves.\n",
    "Note that this will fail if there is no 2-minute cadence light curve for this target.\n",
    "Typically this would be executed using [lightkurve](https://docs.lightkurve.org) directly, but we'll use the MAST API directly because the lightkurve light curve search is a little slow currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary workaround for slow MAST queries with lightkurve\n",
    "observations = Observations.query_criteria(\n",
    "    target_name=f\"{tic}\",\n",
    "    radius=0.0001,\n",
    "    project=[\"TESS\"],\n",
    "    obs_collection=[\"TESS\"],\n",
    "    provenance_name=\"SPOC\",\n",
    "    dataproduct_type=\"timeseries\",\n",
    ")\n",
    "if not len(observations):\n",
    "    raise RuntimeError(\"no 2-minute cadence data\")\n",
    "products = Observations.get_product_list(observations)\n",
    "products = products[products[\"productSubGroupDescription\"] == \"LC\"]\n",
    "files = Observations.download_products(\n",
    "    products, download_dir=tess_world.get_lightkurve_directory()\n",
    ")\n",
    "lcfs = lk.LightCurveCollection(\n",
    "    [lk.open(file).PDCSAP_FLUX for file in files[\"Local Path\"]]\n",
    ")\n",
    "lc = lcfs.stitch().remove_nans()\n",
    "\n",
    "# Extract the data in the correct format\n",
    "x = np.ascontiguousarray(lc.time, dtype=np.float64)\n",
    "y = np.ascontiguousarray(1e3 * (lc.flux - 1), dtype=np.float64)\n",
    "yerr = np.ascontiguousarray(1e3 * lc.flux_err, dtype=np.float64)\n",
    "\n",
    "# Plot the light curve\n",
    "plt.plot(x, y, \"k\", linewidth=0.5)\n",
    "plt.xlabel(\"time [days]\")\n",
    "plt.ylabel(\"relative flux [ppt]\")\n",
    "plt.title(f\"TOI {toi_num}; TIC {tic}\", fontsize=14)\n",
    "\n",
    "# Label the transits on the plot\n",
    "for n in range(num_toi):\n",
    "    t = float(t0_guess[n])\n",
    "    label = f\"TOI {toi_num}.{n + 1:02d}\"\n",
    "    while t < x.max():\n",
    "        plt.axvline(t, color=f\"C{n}\", alpha=0.3, lw=3, label=label)\n",
    "        t += period_guess[n]\n",
    "        label = None\n",
    "\n",
    "plt.xlim(x.min(), x.max())\n",
    "_ = plt.legend(fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make one last adjustment to our initial parameters if any of the TOIs are labeled as a single transit because we'll handle those differently.\n",
    "In particular, for single transits, we'll make the assumption that the period must be *at least* long enough that a second transit could not have occurred in the observational window.\n",
    "This is a strong assumption (because the second transit could have been in a data gap), but it'll do for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with single transits\n",
    "single_transit = period_guess <= 0.0\n",
    "period_guess[single_transit] = x.max() - x.min()\n",
    "period_min = np.maximum(np.abs(t0_guess - x.max()), np.abs(x.min() - t0_guess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we extract just the data near the transits.\n",
    "This helps speed up the analysis and will only limit our precision for stars with extremely coherent variability, and then probably only marginally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transit_mask = np.zeros_like(x, dtype=bool)\n",
    "for n in range(num_toi):\n",
    "    delta = max(1.5 * duration_guess[n], 0.1)\n",
    "    if single_transit[n]:\n",
    "        delta = 1.0\n",
    "    x_fold = (x - t0_guess[n] + 0.5 * period_guess[n]) % period_guess[\n",
    "        n\n",
    "    ] - 0.5 * period_guess[n]\n",
    "    m = np.abs(x_fold) < delta\n",
    "    transit_mask |= m\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.scatter(x_fold[m], y[m], c=x[m], s=3)\n",
    "    plt.xlabel(\"time since transit [days]\")\n",
    "    plt.ylabel(\"relative flux [ppt]\")\n",
    "    plt.colorbar(label=\"time [days]\")\n",
    "    plt.title(\n",
    "        f\"TOI {toi_num}.{n + 1:02d}, PDC flux, period = {period_guess[n]:.3f} d\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "    plt.xlim(-delta, delta)\n",
    "\n",
    "x = np.ascontiguousarray(x[transit_mask])\n",
    "y = np.ascontiguousarray(y[transit_mask])\n",
    "yerr = np.ascontiguousarray(yerr[transit_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The probabilistic model and initialization\n",
    "\n",
    "Here's how we set up the transit model using [exoplanet](https://docs.exoplanet.codes) and [PyMC3](https://docs.pymc.io).\n",
    "For more information about how to use these libraries take a look at the docs that are linked above.\n",
    "In this model, the parameters that we're fitting are:\n",
    "\n",
    "* `mean`: the mean (out-of-transit) flux of the star,\n",
    "* `u`: the quadratic limb darkening parameters, parameterized following [Kipping (2013)](https://arxiv.org/abs/1308.0009)\n",
    "* `sigma`: a jitter parameter that captures excess white noise or underrestimated error bars,\n",
    "* `S_tot`: the total power in a [celerite](https://celerite.readthedocs.io) Gaussian process model (a [SHOTerm](https://docs.exoplanet.codes/en/stable/user/api/#exoplanet.gp.terms.SHOTerm) to be precise) for low-frequency variability,\n",
    "* `ell`: the characteristic time scale of the Gaussian Process model,\n",
    "* `period`: the orbital period with either a log-normal or power-law prior (the latter if the TOI is a single transit),\n",
    "* `t0`: the mid-transit time of a reference transit for each planet,\n",
    "* `transit_depth`: the transit depth in parts per thousand, assuming the small-planet approximation,\n",
    "* `transit_duration`: the transit duration in days, and\n",
    "* `b`: the impact parameter of the orbit (note that this is constrained to the range $0 < b < 1$ so this won't deal gracefully with grazing transits).\n",
    "\n",
    "A few key assumptions include:\n",
    "\n",
    "* The orbits are assumed to be circular, but we fit for both period and duration so this model is flexible enough to fit eccentric orbits to first order (see, for example, [Dawson & Johnson 2012](https://arxiv.org/abs/1203.5537)).\n",
    "* We are neglecting transit times (the ephemeris is assumed to be linear) which should be sufficient for most cases with the short TESS baseline, but transit timing variations could be important for some targets.\n",
    "\n",
    "Finally, note that the model is implemented inside of a model \"factory\" so that we can iteratively clip outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(mask=None):\n",
    "    if mask is None:\n",
    "        mask = np.ones_like(x, dtype=bool)\n",
    "\n",
    "    with pm.Model() as model:\n",
    "        # Stellar parameters\n",
    "        mean = pm.Normal(\"mean\", mu=0.0, sigma=10.0)\n",
    "        u = xo.distributions.QuadLimbDark(\"u\")\n",
    "\n",
    "        # Gaussian process noise model\n",
    "        sigma = pm.InverseGamma(\"sigma\", alpha=3.0, beta=2 * np.median(yerr))\n",
    "        S_tot = pm.Lognormal(\n",
    "            \"S_tot\",\n",
    "            mu=np.log(np.median((y[mask] - np.median(y[mask])) ** 2)),\n",
    "            sigma=5.0,\n",
    "        )\n",
    "        ell = pm.Lognormal(\"ell\", mu=np.log(1.0), sigma=5.0)\n",
    "        Q = 1.0 / 3.0\n",
    "        w0 = 2 * np.pi / ell\n",
    "        S0 = S_tot / (w0 * Q)\n",
    "        kernel = xo.gp.terms.SHOTerm(S0=S0, w0=w0, Q=Q)\n",
    "\n",
    "        # Dealing with period, treating single transits properly\n",
    "        period_params = []\n",
    "        for n in range(num_toi):\n",
    "            if single_transit[n]:\n",
    "                period = pm.Pareto(\n",
    "                    f\"period_{n}\",\n",
    "                    m=period_min[n],\n",
    "                    alpha=2.0 / 3,\n",
    "                    testval=period_guess[n],\n",
    "                )\n",
    "            else:\n",
    "                period = pm.Lognormal(\n",
    "                    f\"period_{n}\", mu=np.log(period_guess[n]), sigma=1.0\n",
    "                )\n",
    "            period_params.append(period)\n",
    "        period = pm.Deterministic(\"period\", tt.stack(period_params))\n",
    "\n",
    "        # Transit parameters\n",
    "        t0 = pm.Normal(\"t0\", mu=t0_guess, sigma=1.0, shape=num_toi)\n",
    "        depth = pm.Lognormal(\n",
    "            \"transit_depth\", mu=np.log(depth_guess), sigma=5.0, shape=num_toi\n",
    "        )\n",
    "        duration = pm.Lognormal(\n",
    "            \"transit_duration\", mu=np.log(duration_guess), sigma=5.0, shape=num_toi\n",
    "        )\n",
    "        b = xo.distributions.UnitUniform(\"b\", shape=num_toi)\n",
    "\n",
    "        # Compute the radius ratio from the transit depth, impact parameter, and\n",
    "        # limb darkening parameters making the small-planet assumption\n",
    "        u1 = u[0]\n",
    "        u2 = u[1]\n",
    "        mu = tt.sqrt(1 - b ** 2)\n",
    "        ror = pm.Deterministic(\n",
    "            \"ror\",\n",
    "            tt.sqrt(\n",
    "                1e-3\n",
    "                * depth\n",
    "                * (1 - u1 / 3 - u2 / 6)\n",
    "                / (1 - u1 * (1 - mu) - u2 * (1 - mu) ** 2)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Set up the orbit\n",
    "        orbit = xo.orbits.KeplerianOrbit(period=period, duration=duration, t0=t0, b=b)\n",
    "\n",
    "        # We're going to track the implied density for reasons that will become clear later\n",
    "        pm.Deterministic(\"rho_circ\", orbit.rho_star)\n",
    "\n",
    "        # Set up the mean transit model\n",
    "        star = xo.LimbDarkLightCurve(u)\n",
    "\n",
    "        lc_model = tess_world.LightCurveModels(mean, star, orbit, ror)\n",
    "\n",
    "        # Finally the GP observation model\n",
    "        gp = xo.gp.GP(kernel, x[mask], yerr[mask] ** 2 + sigma ** 2, mean=lc_model)\n",
    "        gp.marginal(\"obs\", observed=y[mask])\n",
    "\n",
    "        # Double check that everything looks good - we shouldn't see any NaNs!\n",
    "        print(model.check_test_point())\n",
    "\n",
    "        # Optimize the model\n",
    "        map_soln = model.test_point\n",
    "        map_soln = xo.optimize(map_soln, [sigma])\n",
    "        map_soln = xo.optimize(map_soln, [mean, depth, b, duration])\n",
    "        map_soln = xo.optimize(map_soln, [sigma, S_tot, ell])\n",
    "        map_soln = xo.optimize(map_soln, [mean, u])\n",
    "        map_soln = xo.optimize(map_soln, period_params)\n",
    "        map_soln = xo.optimize(map_soln)\n",
    "\n",
    "        # Save some of the key parameters\n",
    "        model.map_soln = map_soln\n",
    "        model.lc_model = lc_model\n",
    "        model.gp = gp\n",
    "        model.mask = mask\n",
    "        model.x = x[mask]\n",
    "        model.y = y[mask]\n",
    "        model.yerr = yerr[mask]\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_with_sigma_clipping(sigma=5.0, maxiter=10):\n",
    "    ntot = len(x)\n",
    "    mask = np.ones_like(x, dtype=bool)\n",
    "    pred = np.zeros_like(y)\n",
    "    for i in range(maxiter):\n",
    "        print(f\"Sigma clipping round {i + 1}\")\n",
    "\n",
    "        with build_model(mask) as model:\n",
    "            pred[mask] = xo.eval_in_model(\n",
    "                model.gp.predict() + model.lc_model(x[mask]), model.map_soln\n",
    "            )\n",
    "            if np.any(~mask):\n",
    "                pred[~mask] = xo.eval_in_model(\n",
    "                    model.gp.predict(x[~mask]) + model.lc_model(x[~mask]),\n",
    "                    model.map_soln,\n",
    "                )\n",
    "\n",
    "        resid = y - pred\n",
    "        rms = np.sqrt(np.median(resid ** 2))\n",
    "        mask = np.abs(resid) < sigma * rms\n",
    "\n",
    "        print(\n",
    "            f\"... clipping {(~mask).sum()} of {len(x)} ({100 * (~mask).sum() / len(x):.1f}%)\"\n",
    "        )\n",
    "\n",
    "        if ntot == mask.sum():\n",
    "            break\n",
    "        ntot = mask.sum()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model_with_sigma_clipping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a visualization of the probabilistic model so that we can see the conditional dependencies between the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after building the model, clipping outliers, and optimizing to estimate the maximum a posteriori (MAP) parameters, we can visualize our initial fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    gp_pred, lc_pred = xo.eval_in_model(\n",
    "        [model.gp.predict(), model.lc_model.light_curves(model.x)], model.map_soln\n",
    "    )\n",
    "\n",
    "for n in range(num_toi):\n",
    "    t0 = model.map_soln[\"t0\"][n]\n",
    "    period = model.map_soln[\"period\"][n]\n",
    "    x_fold = (model.x - t0 + 0.5 * period) % period - 0.5 * period\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.scatter(x_fold, model.y - gp_pred - model.map_soln[\"mean\"], c=model.x, s=3)\n",
    "\n",
    "    inds = np.argsort(x_fold)\n",
    "    plt.plot(x_fold[inds], lc_pred[inds, n], \"k\")\n",
    "\n",
    "    plt.xlabel(\"time since transit [days]\")\n",
    "    plt.ylabel(\"de-trended flux [ppt]\")\n",
    "    plt.colorbar(label=\"time [days]\")\n",
    "    plt.title(\n",
    "        f\"TOI {toi_num}.{n + 1:02d}, map model, period = {period:.3f} d\", fontsize=14\n",
    "    )\n",
    "    delta = max(1.5 * duration_guess[n], 0.1)\n",
    "    if single_transit[n]:\n",
    "        delta = 1.0\n",
    "    plt.xlim(-delta, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this figure, the colors represent the time at which the datapoint was collected.\n",
    "This can show us if a subset of the transits are systematically off.\n",
    "\n",
    "## Inference\n",
    "\n",
    "Now we can get to the good stuff and fit our transit light curve using this probabilistic model and PyMC3's support for Markov chain Monte Carlo (MCMC).\n",
    "The settings here have been chosen to be reasonably sensible defaults in most cases, but you might be able to get better perfomance for your favorite system by tweaking them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(toi_num)\n",
    "with model:\n",
    "    trace = pm.sample(\n",
    "        tune=2000,\n",
    "        draws=2000,\n",
    "        start=model.map_soln,\n",
    "        chains=2,\n",
    "        cores=2,\n",
    "        step=xo.get_dense_nuts_step(target_accept=0.9),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running an MCMC, it's good to look at some summary statistics to see how things went.\n",
    "If any of the `r_hat` values are greater than `1` or if any of the entries in the `ess_mean` column are very small, that suggests that something went wrong while sampling (maybe it's a grazing transit?).\n",
    "Another thing to look for is the number of \"divergences\" listed above.\n",
    "In most cases, there won't be any divergences, but when the model assumptions are not satisfied, this number can indicate that something went wrong.\n",
    "If there were more than about 10 divergences per chain, you should be cautious about the results.\n",
    "Again, this often suggests that the transit shape is consistent with a grazing transit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, before we go any further, let's save the results of the MCMC to disk so that we don't lose them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = pathlib.Path(os.environ.get(\"OUTPUT_DIRECTORY\", \".\"))\n",
    "\n",
    "# Save the model\n",
    "with open(output_directory / \"model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f, -1)\n",
    "\n",
    "# Save the MAP solution\n",
    "with open(output_directory / \"map_soln.json\", \"w\") as f:\n",
    "    json.dump(model.map_soln, f, indent=2, cls=tess_world.NumpyEncoder)\n",
    "\n",
    "# Save the summary statistics\n",
    "summary = pm.summary(trace, round_to=\"none\")\n",
    "summary.to_csv(output_directory / \"summary.csv\")\n",
    "\n",
    "# Save the trace\n",
    "df = pm.trace_to_dataframe(trace, include_transformed=True)\n",
    "stats = pd.DataFrame(\n",
    "    dict((name, trace.get_sampler_stats(name)) for name in trace.stat_names)\n",
    ")\n",
    "with h5py.File(output_directory / \"trace.h5\", \"w\") as f:\n",
    "    f.create_dataset(\"trace\", data=df.to_records(index=False))\n",
    "    f.create_dataset(\"stats\", data=stats.to_records(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Finally, we can look at some of the results of our inference.\n",
    "One of the key figures is the posterior distribution of transit models overplotted on the light curve.\n",
    "Here we're removing the MAP prediction from the Gaussian Process and then plotting 100 random transit models from the chain to get a sense for the uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=num_toi, figsize=(8, 3 * num_toi), squeeze=False)\n",
    "axes = axes[:, 0]\n",
    "\n",
    "for n in range(num_toi):\n",
    "    t0 = np.median(trace[\"t0\"][:, n])\n",
    "    period = np.median(trace[\"period\"][:, n])\n",
    "    x_fold = (model.x - t0 + 0.5 * period) % period - 0.5 * period\n",
    "\n",
    "    delta = max(1.5 * duration_guess[n], 0.1)\n",
    "    if single_transit[n]:\n",
    "        delta = 1.0\n",
    "\n",
    "    m = np.abs(x_fold) < delta\n",
    "    x0 = 24 * x_fold[m]\n",
    "    y0 = (model.y - gp_pred - np.median(trace[\"mean\"]))[m]\n",
    "    axes[n].plot(x0, y0, \".k\", label=\"data\", alpha=0.3, mec=\"none\")\n",
    "\n",
    "    bins = np.linspace(-24 * delta, 24 * delta, 36)\n",
    "    num, _ = np.histogram(x0, bins, weights=y0)\n",
    "    denom, _ = np.histogram(x0, bins)\n",
    "    axes[n].plot(0.5 * (bins[1:] + bins[:-1]), num / denom, \"ok\", label=\"binned data\")\n",
    "\n",
    "    axes[n].set_ylabel(\"de-trended flux [ppt]\")\n",
    "    axes[n].set_xlim(-24 * delta, 24 * delta)\n",
    "\n",
    "ylim = [ax.get_ylim() for ax in axes]\n",
    "\n",
    "with model:\n",
    "    period = np.median(trace[\"period\"], axis=0)\n",
    "    func = xo.utils.get_theano_function_for_var(model.lc_model.light_curves(model.x))\n",
    "    labels = [\n",
    "        f\"TOI {toi_num}.{n + 1:02d}, P = {period[n]:.3f} d\" for n in range(num_toi)\n",
    "    ]\n",
    "    for point in xo.utils.get_samples_from_trace(trace, size=100):\n",
    "        lcs = func(*xo.utils.get_args_for_theano_function(point))\n",
    "        for n in range(num_toi):\n",
    "            t0 = point[\"t0\"][n]\n",
    "            period = point[\"period\"][n]\n",
    "            x_fold = (model.x - t0 + 0.5 * period) % period - 0.5 * period\n",
    "            inds = np.argsort(x_fold)\n",
    "            axes[n].plot(\n",
    "                24 * x_fold[inds],\n",
    "                lcs[inds, n],\n",
    "                f\"C{n}\",\n",
    "                alpha=0.1,\n",
    "                lw=0.75,\n",
    "                label=labels[n],\n",
    "            )\n",
    "            labels[n] = None\n",
    "\n",
    "[ax.legend(fontsize=10, loc=3) for ax in axes]\n",
    "axes[-1].set_xlabel(\"time since transit [hours]\")\n",
    "axes[0].set_title(f\"TOI {toi_num}, posterior inference\", fontsize=14)\n",
    "_ = [axes[n].set_ylim(ylim[n]) for n in range(num_toi)]\n",
    "\n",
    "fig.savefig(output_directory / \"figure.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have the posterior constraint on the period (or periods, if there are multiple TOIs).\n",
    "For display purposes, it is useful to plot the distribution of the difference (measured in minutes) between the sampled period and a fiducial period (the posterior median in this case).\n",
    "If any of the TOIs just have a single transit, we plot the logarithm of the period in days instead since it won't be well constrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_period = np.median(trace[\"period\"], axis=0)\n",
    "samples = np.array(trace[\"period\"])\n",
    "samples[:, ~single_transit] = (\n",
    "    24 * 60 * (samples[:, ~single_transit] - median_period[None, ~single_transit])\n",
    ")\n",
    "samples[:, single_transit] = np.log10(samples[:, single_transit])\n",
    "labels = [\n",
    "    f\"$\\log_{{10}} P_{n + 1} / \\mathrm{{day}}$\"\n",
    "    if single_transit[n]\n",
    "    else f\"$\\Delta P_{n + 1}$ [min]\"\n",
    "    for n in range(num_toi)\n",
    "]\n",
    "\n",
    "fig = corner.corner(samples, labels=labels)\n",
    "for n, ax in enumerate(np.diag(np.array(fig.axes).reshape(num_toi, num_toi))):\n",
    "    if single_transit[n]:\n",
    "        continue\n",
    "    ax.set_title(f\"$P_\\mathrm{{ref}} = {median_period[n]:.6f}$\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other physical parameters that have relevant covariances are the radius ratio, the impact parameter, and the transit duration.\n",
    "(Remember that we're fitting in transit depth, not radius ratio, so the ratio is a derived quantity.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.concatenate(\n",
    "    (trace[\"ror\"], trace[\"b\"], trace[\"transit_duration\"] * 24,), axis=1\n",
    ")\n",
    "labels = [f\"$R_{n + 1} / R_S$\" for n in range(num_toi)]\n",
    "labels += [f\"$b_{n + 1}$\" for n in range(num_toi)]\n",
    "labels += [f\"$\\\\tau_{n + 1}$ [hr]\" for n in range(num_toi)]\n",
    "\n",
    "_ = corner.corner(samples, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are all the plots that we'll make for now, but remember that you can use the samples that we save above to make more figures after the fact if you find something that you want to see.\n",
    "\n",
    "## Attribution and environment\n",
    "\n",
    "If you use these results or this code, please consider citing the relevant sources.\n",
    "First, you can [cite the lightkurve package](https://docs.lightkurve.org/about/citing.html):\n",
    "\n",
    "```bibtex\n",
    "@misc{lightkurve,\n",
    "   author = {{Lightkurve Collaboration} and {Cardoso}, J.~V.~d.~M. and\n",
    "             {Hedges}, C. and {Gully-Santiago}, M. and {Saunders}, N. and\n",
    "             {Cody}, A.~M. and {Barclay}, T. and {Hall}, O. and\n",
    "             {Sagear}, S. and {Turtelboom}, E. and {Zhang}, J. and\n",
    "             {Tzanidakis}, A. and {Mighell}, K. and {Coughlin}, J. and\n",
    "             {Bell}, K. and {Berta-Thompson}, Z. and {Williams}, P. and\n",
    "             {Dotson}, J. and {Barentsen}, G.},\n",
    "    title = \"{Lightkurve: Kepler and TESS time series analysis in Python}\",\n",
    " keywords = {Software, NASA},\n",
    "howpublished = {Astrophysics Source Code Library},\n",
    "     year = 2018,\n",
    "    month = dec,\n",
    "archivePrefix = \"ascl\",\n",
    "   eprint = {1812.013},\n",
    "   adsurl = {http://adsabs.harvard.edu/abs/2018ascl.soft12013L},\n",
    "}\n",
    "```\n",
    "\n",
    "You can also [cite the exoplanet project and its dependencies](https://docs.exoplanet.codes/en/stable/tutorials/citation/) using the following acknowledgement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    txt, bib = xo.citations.get_citations_for_model()\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the citations reference the BibTeX entries in the `bib` variable.\n",
    "See the [tess.world citation page](https://tess.world/citation) for more information. \n",
    "\n",
    "Finally, this notebook was executed in a [conda environment](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) with the following environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env export | grep -v \"name:\" | grep -v \"prefix:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
